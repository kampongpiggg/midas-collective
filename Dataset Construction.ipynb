{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a2f407-030f-42ba-8b3b-add892a607d6",
   "metadata": {},
   "source": [
    "# Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873210e1-7c5e-487d-90f6-a7c11d95cb3a",
   "metadata": {},
   "source": [
    "This Notebook uses SEC's free EDGAR API to pull fundamental data and IBKR's free TWS API to pull price data. We then calculate the various Value, Quality and Momentum metrics and output it in a full_factor_table.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08375fdb-2efe-48be-bac2-f8ac30eff867",
   "metadata": {},
   "source": [
    "## Section 1: Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039fb12-7758-4815-95e4-b6f5612a58b6",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries and Stock Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257ebb4f-9f79-4148-ba7f-927359b8a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from pathlib import Path\n",
    "from ib_insync import IB, Stock, util\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import ttest_1samp\n",
    "from typing import Optional\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c6916d-6d1a-4c8e-b5d5-9d14fc9ec1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── 1. Settings ────────────────────────────────────────────────\n",
    "EDGAR_API_KEY    = 'fc6b859f477a845e0b2a214bfc4cddf78c86f92d5d764845367b6484e7b60d41'\n",
    "START_DATE     = '2010-01-01'\n",
    "END_DATE       = '2025-05-31 23:59:59'\n",
    "locale.setlocale(locale.LC_ALL, '')  # use your locale, e.g. 'en_US.UTF-8'\n",
    "\n",
    "top100_symbols = [\n",
    "    \"MSFT\",\"AAPL\",\"NVDA\",\"AMZN\",\"AVGO\",\"TSLA\",\"GOOGL\", \"APP\", \"MO\",\n",
    "    \"JPM\",\"V\",\"LLY\",\"NFLX\",\"XOM\",\"MA\",\"COST\",\"WMT\",\"PG\",\"HD\", \"BMY\",\n",
    "    \"JNJ\",\"ABBV\",\"BAC\",\"UNH\",\"CRM\",\"KO\",\"ORCL\",\"PM\",\"WFC\", \"INTC\",\n",
    "    \"CSCO\",\"IBM\",\"CVX\",\"GE\",\"ABT\",\"MCD\",\"NOW\",\"ACN\",\"DIS\", \"COF\", \"PH\",\n",
    "    \"MRK\",\"UBER\",\"T\",\"GS\",\"INTU\",\"AMD\",\"VZ\",\"PEP\",\"BKNG\", \"CEG\", \"CVS\",\n",
    "    \"RTX\",\"ADBE\",\"TXN\",\"CAT\",\"AXP\",\"QCOM\",\"PGR\",\"TMO\",\"SPGI\",\"MS\",\n",
    "    \"BA\",\"BSX\",\"NEE\",\"TJX\",\"SCHW\",\"AMGN\",\"HON\",\"C\",\"AMAT\", \"NEM\",\n",
    "    \"UNP\",\"SYK\",\"CMCSA\",\"ETN\",\"LOW\",\"PFE\",\"GILD\",\"DE\",\"DHR\", \"LMT\",\n",
    "    \"ADP\",\"COP\",\"GEV\",\"TMUS\",\"ADI\",\"MMC\",\"LRCX\",\"MDT\", \"HCA\",\n",
    "    \"MU\",\"CB\",\"KLAC\",\"APH\",\"ANET\",\"ICE\",\"SBUX\", \"CMCSA\", \"MCK\"\n",
    "]\n",
    "\n",
    "iwm_top100 = [\n",
    "    \"BE\",\"CRDO\",\"FN\",\"IONQ\",\"SATS\",\"NXT\",\"KTOS\",\"GH\",\"HL\",\"BBIO\",\n",
    "    \"RVMD\",\"OKLO\",\"CDE\",\"SPXC\",\"MDGL\",\"U\",\"DY\",\"STRL\",\"ENSG\",\"GTLS\",\n",
    "    \"IDCC\",\"SANM\",\"UMBF\",\"MOD\",\"QBTS\",\"RGTI\",\"AVAV\",\"ALKS\",\"HQY\",\"DNA\",\n",
    "    \"UMPS\",\"STEM\",\"CMC\",\"IPAR\",\"TTMI\",\"AKRV\",\"WNS\",\"CTRS\",\"AIR\",\"PUB\",\n",
    "    \"PLR\",\"RXA\",\"AXGN\",\"HIMS\",\"ZWS\",\"JDGY\",\"CVLT\",\"ORA\",\"SMTC\",\"PS\",\n",
    "    \"CADE\",\"IBP\",\"JXN\",\"APLD\",\"FCFS\",\"PIPR\",\"BOOT\",\"BNT\",\"TND\",\"EAT\",\n",
    "    \"CWST\",\"MIR\",\"TMHC\",\"RYTM\",\"KSPI\",\"GDS\",\"GATX\",\"BRKT\",\"ASGN\",\"LIC\",\n",
    "    \"LEO\",\"VLY\",\"PTCT\",\"INIC\",\"HWC\",\"TOWN\",\"GBCI\",\"LXP\",\"BIPC\",\"LNG\",\n",
    "    \"QLYS\",\"CVLT\",\"COST\",\"PRAE\",\"HOMA\",\"CNC\",\"RIOT\",\"ACA\",\"OPCH\",\"BMI\",\n",
    "    \"BXC\",\"GPI\",\"INTC\",\"CWAN\",\"PGNY\",\"KNSL\",\"ACHR\",\"POR\",\"BJH\"\n",
    "]\n",
    "\n",
    "sp500_tickers = top100_symbols\n",
    "len(sp500_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224b9a9-dd73-4717-8879-c725ee183ca8",
   "metadata": {},
   "source": [
    "### 1.2 Fetching Fundamentals Data from SEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09041e-d9f6-418f-a202-e1842edd0c9f",
   "metadata": {},
   "source": [
    "For each ticker, we find its Central Index Key (CIK) and then use that to query the SEC database. We fetch quarterly fundamental data through the EDGAR API using the XRBL tags. This is done on a trailing-twelve-month basis so as to deal with the different financial statement filing types (10Q vs 10K). Moreover, as financial statement items can have different names in different companies, we query a wide dictionary of similar words.\n",
    "\n",
    "We then construct monthly windows as of the first trading day (approx. first business day) of each month, using the nearest *prior* TTM fundamentals for each ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd19c1c5-1306-47a8-a75a-7f6177260cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_cik_from_ticker(\n",
    "    ticker: str,\n",
    "    user_agent: str,\n",
    "    api_key: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Resolve ticker -> CIK using SEC's company_tickers.json file.\n",
    "    Returns a zero-padded 10-digit string.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    if api_key:\n",
    "        headers[\"X-API-KEY\"] = api_key\n",
    "\n",
    "    resp = requests.get(base_url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    ticker = ticker.upper()\n",
    "    cik_str = None\n",
    "    for entry in data.values():\n",
    "        if entry.get(\"ticker\", \"\").upper() == ticker:\n",
    "            cik_int = entry[\"cik_str\"]\n",
    "            cik_str = str(cik_int).zfill(10)\n",
    "            break\n",
    "\n",
    "    if cik_str is None:\n",
    "        raise ValueError(f\"Could not find CIK for ticker {ticker}\")\n",
    "\n",
    "    return cik_str\n",
    "\n",
    "\n",
    "def fetch_edgar_fundamentals(\n",
    "    ticker: str,\n",
    "    user_agent: str,\n",
    "    api_key: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch fundamentals from SEC EDGAR companyfacts API and return\n",
    "    a quarterly-only dataframe with TTM metrics (using EBIT instead of EBITDA).\n",
    "    `period_type` is removed from the final output.\n",
    "    \"\"\"\n",
    "    # --- 1) Resolve ticker -> CIK ---\n",
    "    cik_str = _get_cik_from_ticker(ticker, user_agent=user_agent, api_key=api_key)\n",
    "\n",
    "    # --- 2) Fetch companyfacts JSON ---\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik_str}.json\"\n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent,\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    if api_key:\n",
    "        headers[\"X-API-KEY\"] = api_key\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    facts = data.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "\n",
    "    # --- 3) Tag mapping (EXPANDED DEBT + MARGINS) ---\n",
    "    tag_map = {\n",
    "        \"total_equity\": [\n",
    "            \"StockholdersEquity\",\n",
    "            \"StockholdersEquityIncludingPortionAttributableToNoncontrollingInterest\",\n",
    "            \"Equity\",\n",
    "            \"PartnersCapital\",\n",
    "            \"MembersEquity\",\n",
    "        ],\n",
    "\n",
    "        # >>> EXPANDED LONG-TERM DEBT (NONCURRENT) <<<\n",
    "        \"long_term_debt_noncurrent\": [\n",
    "            \"LongTermDebtNoncurrent\",\n",
    "            \"DebtNoncurrent\",\n",
    "            \"LongTermBorrowings\",\n",
    "            \"LongTermDebtAndCapitalLeaseObligations\",\n",
    "            \"LongTermDebtAndFinanceLeaseObligations\",\n",
    "            \"LongTermDebt\",\n",
    "            \"LongTermLoans\",\n",
    "            \"NotesPayableNoncurrent\",\n",
    "            \"LoansPayableNoncurrent\",\n",
    "            \"ConvertibleDebtNoncurrent\",\n",
    "            \"UnsecuredDebtNoncurrent\",\n",
    "            \"SecuredDebtNoncurrent\",\n",
    "            \"SeniorLongTermDebt\",\n",
    "            \"SubordinatedLongTermDebt\",\n",
    "            \"MortgageNotesPayableNoncurrent\",\n",
    "            \"DebenturesNoncurrent\",\n",
    "            \"CommercialPaperNoncurrent\",\n",
    "            \"FinanceLeaseLiabilityNoncurrent\",\n",
    "            \"CapitalLeaseObligationsNoncurrent\",\n",
    "            \"DebtInstrumentCarryingAmount\",\n",
    "            \"LongTermPortionOfDebt\",\n",
    "            \"LongTermPortionOfBorrowings\",\n",
    "        ],\n",
    "\n",
    "        # >>> EXPANDED CURRENT DEBT <<<\n",
    "        \"long_term_debt_current\": [\n",
    "            \"LongTermDebtCurrent\",\n",
    "            \"DebtCurrent\",\n",
    "            \"CurrentPortionOfLongTermDebt\",\n",
    "            \"CurrentPortionOfLongTermBorrowings\",\n",
    "            \"CurrentPortionOfNotesPayable\",\n",
    "            \"CurrentPortionOfDebt\",\n",
    "            \"CurrentPortionOfBorrowings\",\n",
    "            \"CurrentPortionOfBankLoans\",\n",
    "            \"CurrentPortionOfConvertibleDebt\",\n",
    "            \"CurrentPortionOfFinanceLeaseLiability\",\n",
    "            \"CurrentPortionOfCapitalLeaseObligation\",\n",
    "            \"ShortTermBorrowings\",\n",
    "            \"ShortTermNotesPayable\",\n",
    "            \"BankOverdrafts\",\n",
    "            \"CommercialPaper\",\n",
    "            \"DebtCurrentExcludingFinanceLeases\",\n",
    "        ],\n",
    "\n",
    "        \"cash_and_equivalents\": [\n",
    "            \"CashAndCashEquivalentsAtCarryingValue\",\n",
    "            \"CashCashEquivalentsAndShortTermInvestments\",\n",
    "            \"CashAndCashEquivalents\",\n",
    "        ],\n",
    "\n",
    "        \"revenue\": [\n",
    "            \"RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "            \"Revenues\",\n",
    "            \"Revenue\",\n",
    "            \"SalesRevenueNet\",\n",
    "            \"SalesRevenueGoodsNet\",\n",
    "            \"ProductRevenue\",\n",
    "            \"ServiceRevenue\",\n",
    "        ],\n",
    "\n",
    "        # >>> EXPANDED GROSS PROFIT TAGS <<<\n",
    "        \"gross_profit\": [\n",
    "            \"GrossProfit\",\n",
    "            \"GrossProfitLoss\",\n",
    "            \"OperatingRevenueLessCostOfRevenue\",\n",
    "            \"RevenuesLessCostOfSales\",\n",
    "        ],\n",
    "\n",
    "        # >>> EXPANDED OPERATING INCOME TAGS (EBIT PROXY) <<<\n",
    "        \"operating_income\": [\n",
    "            \"OperatingIncomeLoss\",\n",
    "            \"IncomeLossFromOperations\",\n",
    "            \"OperatingProfitLoss\",\n",
    "            \"OperatingProfit\",\n",
    "            \"IncomeFromOperations\",\n",
    "            \"OperatingIncomeLossContinuingOperations\",\n",
    "            \"OperatingIncomeLossContinuingOperationsAndIncomeLossFromEquityMethodInvestments\",\n",
    "            # fallback: pre-tax income from continuing ops if no clean operating income tag\n",
    "            \"IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest\",\n",
    "        ],\n",
    "\n",
    "        \"net_income\": [\n",
    "            \"NetIncomeLoss\",\n",
    "            \"ProfitLoss\",\n",
    "            \"NetIncomeLossAvailableToCommonStockholdersBasic\",\n",
    "        ],\n",
    "\n",
    "        # we keep these tags in case you want them later, but we won't build EBITDA anymore\n",
    "        \"dep_amort\": [\n",
    "            \"DepreciationDepletionAndAmortization\",\n",
    "            \"DepreciationAndAmortization\",\n",
    "            \"DepreciationAndAmortizationOfPropertyPlantAndEquipment\",\n",
    "        ],\n",
    "\n",
    "        \"interest_expense\": [\n",
    "            \"InterestExpense\",\n",
    "            \"InterestAndDebtExpense\",\n",
    "            \"InterestExpenseBorrowings\",\n",
    "        ],\n",
    "\n",
    "        \"income_tax_expense\": [\n",
    "            \"IncomeTaxExpenseBenefit\",\n",
    "            \"IncomeTaxExpense\",\n",
    "            \"IncomeTaxProvision\",\n",
    "        ],\n",
    "\n",
    "        \"shares_outstanding\": [\n",
    "            \"CommonStockSharesOutstanding\",\n",
    "            \"EntityCommonStockSharesOutstanding\",\n",
    "            \"WeightedAverageNumberOfSharesOutstandingBasic\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # --- Raw period collector ---\n",
    "    periods = {}\n",
    "\n",
    "    def _add_fact_series(field_name, tag_candidates, unit_preference=(\"USD\", \"shares\")):\n",
    "        for tag in tag_candidates:\n",
    "            if tag not in facts:\n",
    "                continue\n",
    "            units_dict = facts[tag].get(\"units\", {})\n",
    "            for unit in unit_preference:\n",
    "                if unit not in units_dict:\n",
    "                    continue\n",
    "\n",
    "                for item in units_dict[unit]:\n",
    "                    end = item.get(\"end\")\n",
    "                    val = item.get(\"val\")\n",
    "                    form = item.get(\"form\", \"\")\n",
    "                    fp   = item.get(\"fp\", \"\")\n",
    "                    fy   = item.get(\"fy\", None)\n",
    "\n",
    "                    if end is None or val is None:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        dt = datetime.fromisoformat(end)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    key = dt.date()\n",
    "                    if key not in periods:\n",
    "                        periods[key] = {\n",
    "                            \"fiscal_period_end\": dt.date(),\n",
    "                            \"form\": form,\n",
    "                            \"fp\": fp,\n",
    "                            \"fy\": fy,\n",
    "                        }\n",
    "\n",
    "                    if field_name not in periods[key] or periods[key][field_name] is None:\n",
    "                        periods[key][field_name] = val\n",
    "\n",
    "                break\n",
    "\n",
    "    # Fill all fields\n",
    "    for field_name, tag_candidates in tag_map.items():\n",
    "        if field_name == \"shares_outstanding\":\n",
    "            _add_fact_series(field_name, tag_candidates, unit_preference=(\"shares\", \"SHARES\"))\n",
    "        else:\n",
    "            _add_fact_series(field_name, tag_candidates, unit_preference=(\"USD\",))\n",
    "\n",
    "    if not periods:\n",
    "        raise ValueError(\"No EDGAR facts found for this ticker.\")\n",
    "\n",
    "    df = pd.DataFrame(list(periods.values()))\n",
    "    df[\"fiscal_period_end\"] = pd.to_datetime(df[\"fiscal_period_end\"])\n",
    "\n",
    "    # --- Total Debt ---\n",
    "    lt_non = df.get(\"long_term_debt_noncurrent\", np.nan)\n",
    "    lt_cur = df.get(\"long_term_debt_current\", np.nan)\n",
    "    df[\"total_debt\"] = lt_non.fillna(0) + lt_cur.fillna(0)\n",
    "    df.loc[df[\"total_debt\"] == 0, \"total_debt\"] = np.nan\n",
    "\n",
    "    # --- Determine Period Type ---\n",
    "    def _ptype(r):\n",
    "        fp = str(r.get(\"fp\", \"\")).upper()\n",
    "        form = str(r.get(\"form\", \"\")).upper()\n",
    "        if fp == \"FY\" or form in (\"10-K\", \"20-F\", \"40-F\"):\n",
    "            return \"annual\"\n",
    "        return \"quarterly\"\n",
    "\n",
    "    df[\"period_type\"] = df.apply(_ptype, axis=1)\n",
    "    df[\"fy\"] = df[\"fy\"].fillna(df[\"fiscal_period_end\"].dt.year)\n",
    "\n",
    "    # --- EBIT construction (from operating_income) ---\n",
    "    # Treat operating_income as EBIT proxy; if missing, ebit = NaN\n",
    "    if \"operating_income\" not in df.columns:\n",
    "        df[\"operating_income\"] = np.nan\n",
    "    df[\"ebit\"] = df[\"operating_income\"].astype(float)\n",
    "\n",
    "    # --- Q1–Q4 Construction ---\n",
    "    # Now using EBIT instead of EBITDA in the flow set\n",
    "    flow_cols  = [\"revenue\", \"gross_profit\", \"operating_income\", \"ebit\", \"net_income\"]\n",
    "    stock_cols = [\"total_equity\", \"total_debt\", \"cash_and_equivalents\", \"shares_outstanding\"]\n",
    "\n",
    "    df_q = df[df[\"period_type\"] == \"quarterly\"].copy()\n",
    "    df_fy = df[df[\"period_type\"] == \"annual\"].copy()\n",
    "\n",
    "    q4_rows = []\n",
    "\n",
    "    for fy, grp in df_fy.groupby(\"fy\"):\n",
    "        fy_row = grp.sort_values(\"fiscal_period_end\").iloc[-1]\n",
    "        qrows = df_q[df_q[\"fy\"] == fy].sort_values(\"fiscal_period_end\")\n",
    "\n",
    "        qmap = {}\n",
    "        for _, r in qrows.iterrows():\n",
    "            fp = str(r.get(\"fp\", \"\")).upper()\n",
    "            if fp in (\"Q1\", \"Q2\", \"Q3\") and fp not in qmap:\n",
    "                qmap[fp] = r\n",
    "\n",
    "        if len(qmap) < 3 and len(qrows) >= 3:\n",
    "            rt = list(qrows.itertuples())\n",
    "            qmap = {\"Q1\": rt[0], \"Q2\": rt[1], \"Q3\": rt[2]}\n",
    "\n",
    "        if len(qmap) < 3:\n",
    "            continue\n",
    "\n",
    "        q1, q2, q3 = qmap[\"Q1\"], qmap[\"Q2\"], qmap[\"Q3\"]\n",
    "\n",
    "        q4 = {\n",
    "            \"fiscal_period_end\": fy_row[\"fiscal_period_end\"],\n",
    "            \"period_type\": \"quarterly\",\n",
    "            \"fp\": \"Q4\",\n",
    "            \"fy\": fy,\n",
    "        }\n",
    "\n",
    "        for col in stock_cols:\n",
    "            q4[col] = fy_row.get(col, np.nan)\n",
    "\n",
    "        for col in flow_cols:\n",
    "            fyv = fy_row.get(col, np.nan)\n",
    "            if pd.isna(fyv):\n",
    "                q4[col] = np.nan\n",
    "            else:\n",
    "                parts = [getattr(q1, col, np.nan), getattr(q2, col, np.nan), getattr(q3, col, np.nan)]\n",
    "                if any(pd.isna(p) for p in parts):\n",
    "                    q4[col] = np.nan\n",
    "                else:\n",
    "                    q4[col] = float(fyv) - sum(float(p) for p in parts)\n",
    "\n",
    "        q4_rows.append(q4)\n",
    "\n",
    "    if q4_rows:\n",
    "        df_q4 = pd.DataFrame(q4_rows)\n",
    "        df_q = pd.concat([df_q, df_q4], ignore_index=True)\n",
    "\n",
    "    # --- Compute TTM (including ebit_ttm) ---\n",
    "    df_q = df_q.sort_values(\"fiscal_period_end\").reset_index(drop=True)\n",
    "    for col in flow_cols:\n",
    "        df_q[col] = df_q[col].astype(float)\n",
    "        df_q[col + \"_ttm\"] = df_q[col].rolling(window=4, min_periods=4).sum()\n",
    "\n",
    "    # --- Final Output (drop period_type) ---\n",
    "    base_cols = [\n",
    "        \"fiscal_period_end\",\n",
    "        \"total_equity\", \"total_debt\", \"cash_and_equivalents\",\n",
    "        \"revenue\", \"gross_profit\", \"operating_income\", \"ebit\",\n",
    "        \"net_income\", \"shares_outstanding\",\n",
    "    ]\n",
    "    ttm_cols = [c + \"_ttm\" for c in flow_cols]  # includes ebit_ttm\n",
    "\n",
    "    for col in base_cols:\n",
    "        if col not in df_q.columns:\n",
    "            df_q[col] = np.nan\n",
    "\n",
    "    fund_df = df_q[base_cols + ttm_cols].sort_values(\"fiscal_period_end\").reset_index(drop=True)\n",
    "    return fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51976640-20d2-4d6e-abe6-f8d130f8e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching fundamentals for MSFT ...\n",
      "Fetching fundamentals for AAPL ...\n",
      "Fetching fundamentals for NVDA ...\n",
      "Fetching fundamentals for AMZN ...\n",
      "Fetching fundamentals for AVGO ...\n",
      "Fetching fundamentals for TSLA ...\n",
      "Fetching fundamentals for GOOGL ...\n",
      "Fetching fundamentals for APP ...\n",
      "Fetching fundamentals for MO ...\n",
      "Fetching fundamentals for JPM ...\n",
      "Fetching fundamentals for V ...\n",
      "Fetching fundamentals for LLY ...\n",
      "Fetching fundamentals for NFLX ...\n",
      "Fetching fundamentals for XOM ...\n",
      "Fetching fundamentals for MA ...\n",
      "Fetching fundamentals for COST ...\n",
      "Fetching fundamentals for WMT ...\n",
      "Fetching fundamentals for PG ...\n",
      "Fetching fundamentals for HD ...\n",
      "Fetching fundamentals for BMY ...\n",
      "Fetching fundamentals for JNJ ...\n",
      "Fetching fundamentals for ABBV ...\n",
      "Fetching fundamentals for BAC ...\n",
      "Fetching fundamentals for UNH ...\n",
      "Fetching fundamentals for CRM ...\n",
      "Fetching fundamentals for KO ...\n",
      "Fetching fundamentals for ORCL ...\n",
      "Fetching fundamentals for PM ...\n",
      "Fetching fundamentals for WFC ...\n",
      "Fetching fundamentals for INTC ...\n",
      "Fetching fundamentals for CSCO ...\n",
      "Fetching fundamentals for IBM ...\n",
      "Fetching fundamentals for CVX ...\n",
      "Fetching fundamentals for GE ...\n",
      "Fetching fundamentals for ABT ...\n",
      "Fetching fundamentals for MCD ...\n",
      "Fetching fundamentals for NOW ...\n",
      "Fetching fundamentals for ACN ...\n",
      "Fetching fundamentals for DIS ...\n",
      "Fetching fundamentals for COF ...\n",
      "Fetching fundamentals for PH ...\n",
      "Fetching fundamentals for MRK ...\n",
      "Fetching fundamentals for UBER ...\n",
      "Fetching fundamentals for T ...\n",
      "Fetching fundamentals for GS ...\n",
      "Fetching fundamentals for INTU ...\n",
      "Fetching fundamentals for AMD ...\n",
      "Fetching fundamentals for VZ ...\n",
      "Fetching fundamentals for PEP ...\n",
      "Fetching fundamentals for BKNG ...\n",
      "Fetching fundamentals for CEG ...\n",
      "Fetching fundamentals for CVS ...\n",
      "Fetching fundamentals for RTX ...\n",
      "Fetching fundamentals for ADBE ...\n",
      "Fetching fundamentals for TXN ...\n",
      "Fetching fundamentals for CAT ...\n",
      "Fetching fundamentals for AXP ...\n",
      "Fetching fundamentals for QCOM ...\n",
      "Fetching fundamentals for PGR ...\n",
      "Fetching fundamentals for TMO ...\n",
      "Fetching fundamentals for SPGI ...\n",
      "Fetching fundamentals for MS ...\n",
      "Fetching fundamentals for BA ...\n",
      "Fetching fundamentals for BSX ...\n",
      "Fetching fundamentals for NEE ...\n",
      "Fetching fundamentals for TJX ...\n",
      "Fetching fundamentals for SCHW ...\n",
      "Fetching fundamentals for AMGN ...\n",
      "Fetching fundamentals for HON ...\n",
      "Fetching fundamentals for C ...\n",
      "Fetching fundamentals for AMAT ...\n",
      "Fetching fundamentals for NEM ...\n",
      "Fetching fundamentals for UNP ...\n",
      "Fetching fundamentals for SYK ...\n",
      "Fetching fundamentals for CMCSA ...\n",
      "Fetching fundamentals for ETN ...\n",
      "Fetching fundamentals for LOW ...\n",
      "Fetching fundamentals for PFE ...\n",
      "Fetching fundamentals for GILD ...\n",
      "Fetching fundamentals for DE ...\n",
      "Fetching fundamentals for DHR ...\n",
      "Fetching fundamentals for LMT ...\n",
      "Fetching fundamentals for ADP ...\n",
      "Fetching fundamentals for COP ...\n",
      "Fetching fundamentals for GEV ...\n",
      "Fetching fundamentals for TMUS ...\n",
      "Fetching fundamentals for ADI ...\n",
      "Fetching fundamentals for MMC ...\n",
      "Fetching fundamentals for LRCX ...\n",
      "Fetching fundamentals for MDT ...\n",
      "Fetching fundamentals for HCA ...\n",
      "Fetching fundamentals for MU ...\n",
      "Fetching fundamentals for CB ...\n",
      "Fetching fundamentals for KLAC ...\n",
      "Fetching fundamentals for APH ...\n",
      "Fetching fundamentals for ANET ...\n",
      "Fetching fundamentals for ICE ...\n",
      "Fetching fundamentals for SBUX ...\n",
      "Fetching fundamentals for CMCSA ...\n",
      "Fetching fundamentals for MCK ...\n",
      "✅ Combined fundamentals shape: (6595, 16)\n",
      "Saved combined fundamentals to all_fundamentals_ttm.csv\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "USER_AGENT = \"kampongpiggg@gmail.com\"  # <-- replace with your SEC-compliant UA\n",
    "\n",
    "# --- LOOP THROUGH TICKERS AND CONCATENATE FUNDAMENTALS ---\n",
    "all_fund_dfs = []\n",
    "\n",
    "for ticker in sp500_tickers:\n",
    "    try:\n",
    "        print(f\"Fetching fundamentals for {ticker} ...\")\n",
    "        fund_df = fetch_edgar_fundamentals(\n",
    "            ticker=ticker,\n",
    "            user_agent=USER_AGENT,\n",
    "            api_key=EDGAR_API_KEY,\n",
    "        )\n",
    "        fund_df = fund_df.copy()\n",
    "        fund_df[\"ticker\"] = ticker  # tag rows with ticker\n",
    "        all_fund_dfs.append(fund_df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to fetch {ticker}: {e}\")\n",
    "\n",
    "if all_fund_dfs:\n",
    "    all_fund_df = pd.concat(all_fund_dfs, ignore_index=True)\n",
    "    print(\"✅ Combined fundamentals shape:\", all_fund_df.shape)\n",
    "else:\n",
    "    all_fund_df = pd.DataFrame()\n",
    "    print(\"⚠️ No data frames were created; check errors above.\")\n",
    "\n",
    "# (Optional) Save to CSV\n",
    "all_fund_df.to_csv(\"all_fundamentals_ttm.csv\", index=False)\n",
    "print(\"Saved combined fundamentals to all_fundamentals_ttm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff73dcb-4bdd-4307-ba55-3d28debbe7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_monthly_fundamentals(\n",
    "    csv_path: str = \"all_fundamentals_ttm.csv\",\n",
    "    freq: str = \"BMS\"  # Business Month Start = first business day of each month\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load fundamentals and create a monthly 'as-of' panel on the first trading\n",
    "    day (approx. first business day) of each month, using the nearest\n",
    "    *prior* TTM fundamentals for each ticker.\n",
    "\n",
    "    Steps:\n",
    "    1) Read CSV `all_fundamentals_ttm`.\n",
    "    2) Convert fiscal_period_end to datetime.\n",
    "    3) For each ticker:\n",
    "       - Build a date range from min to max fiscal_period_end with freq='BMS'.\n",
    "       - For each monthly date, attach the latest fundamentals row whose\n",
    "         fiscal_period_end <= as-of date (merge_asof with direction='backward').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    monthly_fund_df : DataFrame\n",
    "        Indexed by ['ticker', 'asof_date'].\n",
    "        All original fundamental columns are preserved.\n",
    "    \"\"\"\n",
    "    # 1) Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 2) Convert fiscal_period_end to datetime and sort\n",
    "    df[\"fiscal_period_end\"] = pd.to_datetime(df[\"fiscal_period_end\"])\n",
    "    df = df.sort_values([\"ticker\", \"fiscal_period_end\"])\n",
    "\n",
    "    out_frames = []\n",
    "\n",
    "    # 3) Process each ticker independently\n",
    "    for ticker, grp in df.groupby(\"ticker\"):\n",
    "        grp = grp.sort_values(\"fiscal_period_end\")\n",
    "\n",
    "        # Build monthly index: first business day of each month\n",
    "        start = grp[\"fiscal_period_end\"].min().normalize()\n",
    "        end = grp[\"fiscal_period_end\"].max().normalize()\n",
    "        monthly_idx = pd.date_range(start=start, end=end, freq=freq)\n",
    "\n",
    "        monthly_dates = pd.DataFrame({\"asof_date\": monthly_idx})\n",
    "\n",
    "        # For each as-of date, get the last available fundamentals row\n",
    "        # with fiscal_period_end <= asof_date\n",
    "        merged = pd.merge_asof(\n",
    "            monthly_dates.sort_values(\"asof_date\"),\n",
    "            grp.sort_values(\"fiscal_period_end\"),\n",
    "            left_on=\"asof_date\",\n",
    "            right_on=\"fiscal_period_end\",\n",
    "            direction=\"backward\"\n",
    "        )\n",
    "\n",
    "        merged[\"ticker\"] = ticker\n",
    "        out_frames.append(merged)\n",
    "\n",
    "    monthly_fund_df = pd.concat(out_frames, ignore_index=True)\n",
    "\n",
    "    # Set a clean MultiIndex: (ticker, asof_date)\n",
    "    monthly_fund_df = monthly_fund_df.set_index([\"ticker\", \"asof_date\"]).sort_index()\n",
    "\n",
    "    return monthly_fund_df\n",
    "\n",
    "monthly_fund_df = prepare_monthly_fundamentals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788ee3d-86d9-4626-89be-179cd1d3d43d",
   "metadata": {},
   "source": [
    "### 1.3 Fetching OHLCV Price Data from IBKR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478fb7d-e0c2-4cd0-a0bc-ac7cd69a420a",
   "metadata": {},
   "source": [
    "We use IBKR's historical data and query price data. We fetch OHLCV price data, which includes open, high, low, close and volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e2c027-62c9-45ae-90b0-e616d0df8b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price data pulled: 359217 rows in 11.7 mins\n"
     ]
    }
   ],
   "source": [
    "# ── 4. Fetch Price & Volume from IBKR ─────────────────────────\n",
    "def fetch_ohlcv_ibkr(tickers):\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    ib = IB()\n",
    "    ib.connect('127.0.0.1', 7496, clientId=21)\n",
    "    dfs = []\n",
    "    \n",
    "    for symbol in tickers:\n",
    "        contract = Stock(symbol, 'SMART', 'USD')\n",
    "        bars = ib.reqHistoricalData(\n",
    "            contract,\n",
    "            endDateTime = '',\n",
    "            durationStr='15 Y',\n",
    "            barSizeSetting='1 day',\n",
    "            whatToShow='TRADES',\n",
    "            useRTH=True\n",
    "        )\n",
    "        df = util.df(bars)[['date','open','high','low','close','volume']]\n",
    "        df['ticker'] = symbol\n",
    "        dfs.append(df)\n",
    "    \n",
    "    ib.disconnect()\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "start = time.time()\n",
    "raw_df = fetch_ohlcv_ibkr(sp500_tickers)\n",
    "end   = time.time()\n",
    "print(f\"Price data pulled: {raw_df.shape[0]} rows in {((end-start)/60):.1f} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83398377-b76a-437f-a660-a8e5a284ece1",
   "metadata": {},
   "source": [
    "## Section 2: Feature Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3668e5b-d276-4e17-96a4-608b9d9a09d2",
   "metadata": {},
   "source": [
    "We then computed the different Value, Quality and Momentum metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a33ee5-0a0f-4595-90d0-61dec0036050",
   "metadata": {},
   "source": [
    "### 2.1 Computing Value Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c3f32d-d6a6-4e86-ae85-490819cdead7",
   "metadata": {},
   "source": [
    "We are primarily interested in three Value metrics:\n",
    "* book_to_price &rarr; shows how overvalued / undervalued the companies' assets less liabilities are compared to the market price of the shares.\n",
    "* ev_ebit &rarr; shows how much investors pay per dollar of core earnings\n",
    "* debt_to_equity &rarr; shows how much financial leverage the company's using (correlates to health)\n",
    "\n",
    "We calculate the metrics above manually using the fundamental data extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3cbd38-f10c-42d7-bba5-e661e79d135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_factors(\n",
    "    price_df: pd.DataFrame,\n",
    "    fund_df: pd.DataFrame,\n",
    "    date_range,\n",
    "    report_lag_days: int = 90\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute value factors over `date_range`:\n",
    "      - book_to_price\n",
    "      - ev_ebit (using TTM EBIT)\n",
    "      - debt_to_equity\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : daily price/volume DataFrame\n",
    "               index: DatetimeIndex of trading days\n",
    "               columns: ['close','volume',...]\n",
    "    fund_df  : fundamentals DataFrame\n",
    "               must contain at least:\n",
    "               ['fiscal_period_end','total_equity','total_debt',\n",
    "                'cash_and_equivalents','ebit_ttm','shares_outstanding']\n",
    "    date_range : iterable of datetime-like (rebalance dates)\n",
    "    report_lag_days : int, lag in days to mimic reporting delay\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value_df : DataFrame indexed by date_range with columns:\n",
    "               ['book_to_price','ev_ebit','debt_to_equity']\n",
    "    \"\"\"\n",
    "    date_index = pd.to_datetime(pd.Index(date_range)).sort_values()\n",
    "    value_rows = []\n",
    "\n",
    "    for asof in date_index:\n",
    "        asof = pd.to_datetime(asof)\n",
    "\n",
    "        # --- price as of date (last available) ---\n",
    "        px_idx = price_df.index[price_df.index <= asof]\n",
    "        if len(px_idx) == 0:\n",
    "            value_rows.append({\n",
    "                \"date\": asof,\n",
    "                \"book_to_price\": np.nan,\n",
    "                \"ev_ebit\": np.nan,\n",
    "                \"debt_to_equity\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # force scalar\n",
    "        price_val = price_df.loc[px_idx[-1], \"close\"]\n",
    "        try:\n",
    "            price_val = float(price_val)\n",
    "        except (TypeError, ValueError):\n",
    "            price_val = np.nan\n",
    "\n",
    "        # --- fundamentals with reporting lag ---\n",
    "        cutoff = asof - timedelta(days=report_lag_days)\n",
    "        eligible = fund_df[fund_df[\"fiscal_period_end\"] <= cutoff]\n",
    "\n",
    "        if eligible.empty:\n",
    "            value_rows.append({\n",
    "                \"date\": asof,\n",
    "                \"book_to_price\": np.nan,\n",
    "                \"ev_ebit\": np.nan,\n",
    "                \"debt_to_equity\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        f = eligible.iloc[-1]\n",
    "\n",
    "        # pull & coerce to scalars\n",
    "        def _to_float(x, default=np.nan):\n",
    "            if x is None:\n",
    "                return default\n",
    "            if pd.isna(x):\n",
    "                return default\n",
    "            try:\n",
    "                return float(x)\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        equity_val = _to_float(f.get(\"total_equity\", np.nan))\n",
    "        debt_val   = _to_float(f.get(\"total_debt\", np.nan))\n",
    "        cash_val   = _to_float(f.get(\"cash_and_equivalents\", 0.0))\n",
    "        ebit_ttm   = _to_float(f.get(\"ebit_ttm\", np.nan))\n",
    "        shares_val = _to_float(f.get(\"shares_outstanding\", np.nan))\n",
    "\n",
    "        # --- book_to_price ---\n",
    "        if pd.notna(equity_val) and pd.notna(shares_val) and pd.notna(price_val) and shares_val != 0 and price_val != 0:\n",
    "            book_per_share = equity_val / shares_val\n",
    "            book_to_price = book_per_share / price_val\n",
    "        else:\n",
    "            book_to_price = np.nan\n",
    "\n",
    "        # --- EV / EBIT (TTM) ---\n",
    "        if pd.notna(price_val) and pd.notna(shares_val):\n",
    "            mkt_cap = price_val * shares_val\n",
    "        else:\n",
    "            mkt_cap = np.nan\n",
    "\n",
    "        if pd.notna(mkt_cap) and pd.notna(ebit_ttm) and ebit_ttm != 0:\n",
    "            # treat missing debt/cash as 0 in EV bridge\n",
    "            ev = mkt_cap + (debt_val if pd.notna(debt_val) else 0.0) - (cash_val if pd.notna(cash_val) else 0.0)\n",
    "            ev_ebit = ev / ebit_ttm\n",
    "        else:\n",
    "            ev_ebit = np.nan\n",
    "\n",
    "        # --- debt_to_equity ---\n",
    "        if pd.notna(debt_val) and pd.notna(equity_val) and equity_val != 0:\n",
    "            debt_to_equity = debt_val / equity_val\n",
    "        else:\n",
    "            debt_to_equity = np.nan\n",
    "\n",
    "        value_rows.append({\n",
    "            \"date\": asof,\n",
    "            \"book_to_price\": book_to_price,\n",
    "            \"ev_ebit\": ev_ebit,\n",
    "            \"debt_to_equity\": debt_to_equity,\n",
    "        })\n",
    "\n",
    "    value_df = pd.DataFrame(value_rows).set_index(\"date\").sort_index()\n",
    "    return value_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c2bb7-a93d-44d3-b374-dc4742877029",
   "metadata": {},
   "source": [
    "### 2.2 Compute Quality Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974292f9-d663-4dfb-8c57-989a57926e32",
   "metadata": {},
   "source": [
    "We are primarily interested in three Quality metrics:\n",
    "* gross_margin &rarr; shows the core profitability of the business's products (does not include overheads).\n",
    "* operating_margin &rarr; shows the core profitability of a business's operations (includes overheads).\n",
    "* roe &rarr; shows management's efficiency at generating returns.\n",
    "\n",
    "We calculate the metrics above manually using the fundamental data extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042d0b07-bae0-4c4c-ab87-2d7098f704dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quality_factors(\n",
    "    fund_df: pd.DataFrame,\n",
    "    date_range,\n",
    "    report_lag_days: int = 90\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute quality factors over `date_range`:\n",
    "      - gross_margin  = gross_profit / revenue\n",
    "      - operating_margin = operating_income / revenue\n",
    "      - roe = net_income / total_equity\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fund_df  : fundamentals DataFrame\n",
    "    date_range : iterable of datetime-like\n",
    "    report_lag_days : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quality_df : DataFrame indexed by date_range with columns:\n",
    "                 ['gross_margin','operating_margin','roe']\n",
    "    \"\"\"\n",
    "    date_index = pd.to_datetime(pd.Index(date_range)).sort_values()\n",
    "    quality_rows = []\n",
    "\n",
    "    # small helper to coerce to float scalars\n",
    "    def _to_float(x, default=np.nan):\n",
    "        if x is None:\n",
    "            return default\n",
    "        if pd.isna(x):\n",
    "            return default\n",
    "        try:\n",
    "            return float(x)\n",
    "        except (TypeError, ValueError):\n",
    "            return default\n",
    "\n",
    "    for asof in date_index:\n",
    "        asof = pd.to_datetime(asof)\n",
    "        cutoff = asof - timedelta(days=report_lag_days)\n",
    "        eligible = fund_df[fund_df[\"fiscal_period_end\"] <= cutoff]\n",
    "\n",
    "        if eligible.empty:\n",
    "            quality_rows.append({\n",
    "                \"date\": asof,\n",
    "                \"gross_margin\": np.nan,\n",
    "                \"operating_margin\": np.nan,\n",
    "                \"roe\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        f = eligible.iloc[-1]\n",
    "\n",
    "        # coerce all inputs to clean scalars\n",
    "        revenue       = _to_float(f.get(\"revenue\", np.nan))\n",
    "        gross_profit  = _to_float(f.get(\"gross_profit\", np.nan))\n",
    "        op_inc        = _to_float(f.get(\"operating_income\", np.nan))\n",
    "        net_inc       = _to_float(f.get(\"net_income\", np.nan))\n",
    "        equity        = _to_float(f.get(\"total_equity\", np.nan))\n",
    "\n",
    "        # --- gross_margin ---\n",
    "        if pd.notna(gross_profit) and pd.notna(revenue) and revenue != 0:\n",
    "            gross_margin = gross_profit / revenue\n",
    "        else:\n",
    "            gross_margin = np.nan\n",
    "\n",
    "        # --- operating_margin ---\n",
    "        if pd.notna(op_inc) and pd.notna(revenue) and revenue != 0:\n",
    "            operating_margin = op_inc / revenue\n",
    "        else:\n",
    "            operating_margin = np.nan\n",
    "\n",
    "        # --- ROE ---\n",
    "        if pd.notna(net_inc) and pd.notna(equity) and equity != 0:\n",
    "            roe = net_inc / equity\n",
    "        else:\n",
    "            roe = np.nan\n",
    "\n",
    "        quality_rows.append({\n",
    "            \"date\": asof,\n",
    "            \"gross_margin\": gross_margin,\n",
    "            \"operating_margin\": operating_margin,\n",
    "            \"roe\": roe,\n",
    "        })\n",
    "\n",
    "    quality_df = pd.DataFrame(quality_rows).set_index(\"date\").sort_index()\n",
    "    return quality_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cce43-0d43-4bb0-9b1a-23af5806c36a",
   "metadata": {},
   "source": [
    "### 2.3 Compute Momentum Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72fc291-9e6c-49ed-a925-2541ba007fa5",
   "metadata": {},
   "source": [
    "We are primarily interested in two Momentum metrics:\n",
    "* mom_6m &rarr; shows the raw price trend strength in the last six months (how much has it moved in the last six months?)\n",
    "* obv &rarr; shows volume-confirmed momentum; rising OBV indicates accumulation and buying pressure, while falling OBV indicates distribution and selling pressure.\n",
    "\n",
    "We calculate the metrics above manually using the price data extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ea9ba6-cbbf-4ede-b2a2-a39086fa1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_momentum_factors(\n",
    "    price_df: pd.DataFrame,\n",
    "    date_range,\n",
    "    mom_window_days: int = 126,   # ~6 months of trading days\n",
    "    obv_lookback_days: int = 252  # 1Y for OBV build; can tune\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute momentum factors over `date_range`:\n",
    "      - mom_6m : 6-month price momentum\n",
    "      - obv    : On-Balance Volume (absolute level at as-of date)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    price_df : daily price DataFrame (must have 'close','volume')\n",
    "    date_range : iterable of datetime-like\n",
    "    mom_window_days : lookback window for momentum (in calendar days)\n",
    "    obv_lookback_days : lookback window for OBV construction (in calendar days)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mom_df : DataFrame indexed by date_range with columns:\n",
    "             ['mom_6m','obv']\n",
    "    \"\"\"\n",
    "    price_df = price_df.sort_index()\n",
    "    date_index = pd.to_datetime(pd.Index(date_range)).sort_values()\n",
    "    mom_rows = []\n",
    "\n",
    "    # Precompute OBV over full history once\n",
    "    obv_series = []\n",
    "    obv = 0\n",
    "    prev_close = None\n",
    "    for dt, row in price_df.iterrows():\n",
    "        c = row['close']\n",
    "        v = row['volume']\n",
    "        if prev_close is None:\n",
    "            prev_close = c\n",
    "            obv_series.append((dt, obv))\n",
    "            continue\n",
    "        if c > prev_close:\n",
    "            obv += v\n",
    "        elif c < prev_close:\n",
    "            obv -= v\n",
    "        # else, OBV unchanged\n",
    "        prev_close = c\n",
    "        obv_series.append((dt, obv))\n",
    "\n",
    "    obv_df = pd.DataFrame(obv_series, columns=['dt', 'obv']).set_index('dt')\n",
    "\n",
    "    for asof in date_index:\n",
    "        asof = pd.to_datetime(asof)\n",
    "\n",
    "        # Align to last trading day <= asof\n",
    "        px_idx = price_df.index[price_df.index <= asof]\n",
    "        if len(px_idx) == 0:\n",
    "            mom_rows.append({'date': asof, 'mom_6m': np.nan, 'obv': np.nan})\n",
    "            continue\n",
    "        asof_trading = px_idx[-1]\n",
    "\n",
    "        # --- 6m momentum ---\n",
    "        mom_start = asof_trading - timedelta(days=mom_window_days)\n",
    "        hist_mom = price_df.loc[mom_start:asof_trading]\n",
    "\n",
    "        if len(hist_mom) < 2:\n",
    "            mom_6m = np.nan\n",
    "        else:\n",
    "            p_t = hist_mom['close'].iloc[-1]\n",
    "            p_0 = hist_mom['close'].iloc[0]\n",
    "            mom_6m = p_t / p_0 - 1\n",
    "\n",
    "        # --- OBV at as-of date ---\n",
    "        obv_idx = obv_df.index[obv_df.index <= asof_trading]\n",
    "        if len(obv_idx) == 0:\n",
    "            obv_val = np.nan\n",
    "        else:\n",
    "            obv_val = obv_df.loc[obv_idx[-1], 'obv']\n",
    "\n",
    "        mom_rows.append({\n",
    "            'date': asof,\n",
    "            'mom_6m': mom_6m,\n",
    "            'obv': obv_val\n",
    "        })\n",
    "\n",
    "    mom_df = pd.DataFrame(mom_rows).set_index('date').sort_index()\n",
    "    return mom_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2eb335-21f6-478d-b2f6-0134245e03dc",
   "metadata": {},
   "source": [
    "### 2.4 Building Factor Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b48414-ea20-4892-b7d4-2c6f5d0773c2",
   "metadata": {},
   "source": [
    "We then build the entire factor table by running the three helper functions above in a loop. The result is a concatenated table with all the metrics indexed by (ticker, date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae039a7-fa64-4f5b-aea3-1cf6f5686833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_factor_table(\n",
    "    raw_df: pd.DataFrame,\n",
    "    monthly_fund_df: pd.DataFrame,\n",
    "    tickers=None,\n",
    "    report_lag_days: int = 90,\n",
    "    mom_window_days: int = 126,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build factor tables for all tickers using:\n",
    "      - raw_df: OHLCV for multiple tickers\n",
    "                columns must include ['date','close','volume','ticker']\n",
    "      - monthly_fund_df: fundamentals with MultiIndex ['ticker','asof_date']\n",
    "                         and at least 'fiscal_period_end' plus the\n",
    "                         other fields required by the helpers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factors_all : DataFrame\n",
    "        MultiIndex (ticker, date) with columns:\n",
    "        ['book_to_price','ev_ebitda','debt_to_equity',\n",
    "         'gross_margin','operating_margin','roe',\n",
    "         'mom_6m','obv']\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Ensure date column is datetime -----\n",
    "    raw_df = raw_df.copy()\n",
    "    raw_df[\"date\"] = pd.to_datetime(raw_df[\"date\"])\n",
    "\n",
    "    # ----- Infer tickers if not provided -----\n",
    "    if tickers is None:\n",
    "        tickers_price = set(raw_df[\"ticker\"].unique())\n",
    "\n",
    "        if isinstance(monthly_fund_df.index, pd.MultiIndex) and \"ticker\" in monthly_fund_df.index.names:\n",
    "            tickers_fund = set(monthly_fund_df.index.get_level_values(\"ticker\").unique())\n",
    "        else:\n",
    "            tickers_fund = set(monthly_fund_df[\"ticker\"].unique())\n",
    "\n",
    "        tickers = sorted(tickers_price & tickers_fund)\n",
    "\n",
    "    all_factors = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # ---------- slice price_df for this ticker ----------\n",
    "        px = raw_df[raw_df[\"ticker\"] == ticker].copy()\n",
    "        if px.empty:\n",
    "            continue\n",
    "\n",
    "        price_df = (\n",
    "            px.sort_values(\"date\")\n",
    "              .set_index(\"date\")[[\"close\", \"volume\"]]\n",
    "        )\n",
    "\n",
    "        # date_range comes directly from the price index (daily trading dates)\n",
    "        date_range = pd.to_datetime(price_df.index.unique()).sort_values()\n",
    "\n",
    "        # ---------- slice fund_df for this ticker ----------\n",
    "        if isinstance(monthly_fund_df.index, pd.MultiIndex) and \"ticker\" in monthly_fund_df.index.names:\n",
    "            fd = monthly_fund_df.xs(ticker, level=\"ticker\").copy()\n",
    "        else:\n",
    "            fd = monthly_fund_df[monthly_fund_df[\"ticker\"] == ticker].copy()\n",
    "\n",
    "        if fd.empty:\n",
    "            continue\n",
    "\n",
    "        if \"fiscal_period_end\" not in fd.columns:\n",
    "            raise ValueError(\"monthly_fund_df must contain a 'fiscal_period_end' column.\")\n",
    "\n",
    "        fund_df = fd.reset_index().sort_values(\"fiscal_period_end\")\n",
    "\n",
    "        # ---------- compute factor blocks ----------\n",
    "        value_df = compute_value_factors(\n",
    "            price_df=price_df,\n",
    "            fund_df=fund_df,\n",
    "            date_range=date_range,\n",
    "            report_lag_days=report_lag_days,\n",
    "        )\n",
    "\n",
    "        quality_df = compute_quality_factors(\n",
    "            fund_df=fund_df,\n",
    "            date_range=date_range,\n",
    "            report_lag_days=report_lag_days,\n",
    "        )\n",
    "\n",
    "        mom_df = compute_momentum_factors(\n",
    "            price_df=price_df,\n",
    "            date_range=date_range,\n",
    "            mom_window_days=mom_window_days,\n",
    "        )\n",
    "\n",
    "        # ---------- combine for this ticker ----------\n",
    "        df = pd.concat([value_df, quality_df, mom_df], axis=1)\n",
    "        df[\"ticker\"] = ticker\n",
    "\n",
    "        df = (\n",
    "            df.reset_index()          # 'date' (from helpers) back to column\n",
    "              .set_index([\"ticker\", \"date\"])\n",
    "              .sort_index()\n",
    "        )\n",
    "\n",
    "        all_factors.append(df)\n",
    "\n",
    "    if not all_factors:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"book_to_price\", \"ev_ebitda\", \"debt_to_equity\",\n",
    "                \"gross_margin\", \"operating_margin\", \"roe\",\n",
    "                \"mom_6m\", \"obv\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    factors_all = pd.concat(all_factors, axis=0).sort_index()\n",
    "    return factors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b233be-78d1-4bfe-960f-47af43a299fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factors_table = build_factor_table(\n",
    "    raw_df=raw_df,\n",
    "    monthly_fund_df=monthly_fund_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9f9e1a-c12e-4f8b-81f8-0ff1a044f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved factor table to csv\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Save to CSV\n",
    "factors_table.to_csv(\"full_factors_table.csv\", index=True)\n",
    "print(\"Saved factor table to csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d327a9-a4ad-4b67-ab76-f445f479b31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
